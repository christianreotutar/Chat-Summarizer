%
% File finalreport.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{A Survey of Short-text Classification Using Modified Na{\"i}ve Bayes and Multiclass Support Vector Machines\Thanks{This
    document has been adapted from the instructions for earlier ACL
    and NAACL proceedings, including those for NAACL-HLT-09 by Joakim
    Nivre and Noah Smith, for ACL-05 by Hwee Tou Ng and Kemal Oflazer,
    for ACL-02 by Eugene Charniak and Dekang Lin, and earlier ACL and
    EACL formats.  Those versions were written by several people,
    including John Chen, Henry S. Thompson and Donald Walker.
    Additional elements were taken from the formatting instructions of
    the {\em International Joint Conference on Artificial
      Intelligence}.}}

\author{Michael Peven\\
  Johns Hopkins University\\
  225 W. 29th St\\
  Baltimore, MD 21211, USA\\
  {\tt mpeven1@jhu.edu}
  \And
  Christian Reotutar \\
  Johns Hopkins University \\
  3700 N. Charles St. \\
  Baltimore MD, USA\\
  {\tt creotut1@jhu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Electronic messaging services, including Short Message Service (SMS) over phone networs, Instant Messaging (IM) such as Facebook Chat or Whats App, and microblogging platforms such as Twitter have become more popular than ever before. This massive amount of raw data is unparalled in size and while it may be overwhelming to users, but it gives way to many possibilities for the field of machine learning.
  
  While the majority of all these "short-text" documents are unlabeled, the text posted on the social network known as Twitter (https://twitter.com/) comes naturally labeled through the use of "hashtags" (see section XXXXXXXXXXXXXX). To process the labeled data in this typically multi-class problem, the Na{\"i}ve Bayes classifier is a popular model with results rivaling that of the Support Vector Machine classifier with shorter training time. In this survey, we seek to find improvements in the Na{\"i}ve Bayes classifier such that accuracy is improved and compare this to another classifier used in the problem space.
\end{abstract}

\section{Introduction}
  The classification of documents, spanning from text documents and articles to images and videos, has been a prominent problem in information and computer science. The problem is to assign a category or classification to a given document that gives some indication to relationships between documents or information about the document itself.

\subsection{Problems in Document Classification}
  Most of the difficulty in getting meaningful results of short-text or chat-message topic classification stems from the incorrect grammar and use of slang in electronic human conversations. In full text documents, subject indexing and classification techniques as well as summarization algorithms have shown success due to advancements in natural language processing algorithms. However, the same algorithms do not apply well to human chat messages where there is lack of sentence structure and correct spelling.
  This is why we apply supervised learning techniques at a massive scale rather than carefully selected artificial intelligence algorithms designed through the engineering and reverse-engineering of lexical semantics, setence structure, narrative understanding, and other syntax related theory.

\subsection{Topic Classification}
  This survey deals with the problem of short-text classification in relation to summarization of chat. That is, given a line of text, the goal is to predict the topic of the shart-text (chat message) based on a bag-of-words approach. We do this in the domain of Twitter messages: given a tweet body, we wish to predict the associated hashtag for the tweet. The assumption being that the associated hashtag represents the topic of the tweet.
  


\section{Data Description}
The data we are using for this project comes from Twitter (https://twitter.com/). The tweets were downloaded from the Twitter streaming API v1.1 using Mark Dredze's Twitter Stream Dowloader (https://github.com/mdredze/twitter_stream_downloader).

\section{Machine Learning Techniques}
Here give a broad overview of what we're comparing, what techniques we used and why

\subsection{Na{\"i}ve Bayes}
\textbf{http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf} \textbf{http://www2.hawaii.edu/~chin/702/sigir99.pdf}
  The Na{\"i}ve Bayes Classifier is a probabilistic model based on Bayes' theorem for conditional probabilities but with a na{\"i}ve independence assumption. The basic idea is to find the probability of a classification given the data  (features). This is calculated with probabilities based on the entire training set. The independence assumption assumes the features conditionally independent. For example, features like time of day written and timezone are independent of each other and the presence of one value does highly correlate to the presence of another value. This assumption simplifies calculations and often makes Na{\"i}ve Bayes a quickly trained classifier.

As it relates to our project, we use a bag of words as our features and a single word as the classification for the bag of words. We assume presence of a word is independent of other words. 


\subsection{Modified Na{\"i}ve Bayes}
Here we give background into the classifier + word2vec library

\subsection{SVM}
Here we give background into the classifier

\section{Methods}
\subsection{Data Description}
Here we talk about the filtering done on the data, the importance of feature vector formation and attribute picking, and the problems we came upon (e.g. language, more than one hash tag).

\subsection{Code for Na{\"i}ve Bayes}
  In this survey, we use the multinomial Na{\"i}ve Bayes model used by [Yang and Liu, 1999]

\subsection{Code for Modified Na{\"i}ve Bayes}

\subsection{Code for SVM}

\section{Results}
Here we give a broad overview of our results

\subsection{Comparisons}
Lots of tables and graphs

\subsection{Explanations}
It is important that we relate our results  to the classifiers we used and other ML topics.

\subsection{How we can improve}
Any improvements for the classifiers we did not have time to implement

\section{Conclusion}

\section{Comparison to Proposal}

\begin{itemize}
\item A list of what we did differently from proposal
\end{itemize}

\section*{Acknowledgments}
We would like to acknowledge Ilya Shpitser for giving feedback on our project and offering suggestions. Ilya for the skeleton for the code.
Also, we would like to thank Mark Dredze for providing access to his twitter stream downloader (https://github.com/mdredze/twitter_stream_downloader).

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\end{thebibliography}

\end{document}
